{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456b3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef9d8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏠 Project Root: C:\\Users\\Joseph\\generative-health-models\n",
      "\n",
      "🏗️ Verifying project structure:\n",
      "  ✅ data/raw/wesad\n",
      "  ✅ data/processed\n",
      "  ✅ data/synthetic\n",
      "  ✅ models/tc_multigan\n",
      "  ✅ results/figures\n",
      "  ✅ results/metrics\n",
      "  ✅ src/data\n",
      "  ✅ src/models\n",
      "  ✅ src/evaluation\n",
      "\n",
      "✅ Project setup completed!\n"
     ]
    }
   ],
   "source": [
    "# Define project root - CHANGE THIS TO YOUR PATH\n",
    "project_root = r\"C:\\Users\\Joseph\\generative-health-models\"\n",
    "\n",
    "print(f\"🏠 Project Root: {project_root}\")\n",
    "\n",
    "# Add to Python path for imports\n",
    "sys.path.append(project_root)\n",
    "sys.path.append(os.path.join(project_root, 'src'))\n",
    "\n",
    "# Verify project structure exists\n",
    "required_dirs = [\n",
    "    \"data/raw/wesad\", \n",
    "    \"data/processed\", \n",
    "    \"data/synthetic\",\n",
    "    \"models/tc_multigan\", \n",
    "    \"results/figures\", \n",
    "    \"results/metrics\",\n",
    "    \"src/data\",\n",
    "    \"src/models\",\n",
    "    \"src/evaluation\"\n",
    "]\n",
    "\n",
    "print(\"\\n🏗️ Verifying project structure:\")\n",
    "for dir_path in required_dirs:\n",
    "    full_path = os.path.join(project_root, dir_path)\n",
    "    exists = os.path.exists(full_path)\n",
    "    print(f\"  {'✅' if exists else '❌'} {dir_path}\")\n",
    "    if not exists:\n",
    "        os.makedirs(full_path, exist_ok=True)\n",
    "        print(f\"    📁 Created: {full_path}\")\n",
    "\n",
    "print(f\"\\n✅ Project setup completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e4914d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "WESAD DATASET STRUCTURE ANALYSIS (CORRECTED)\n",
      "============================================================\n",
      "📁 Found 15 subject files:\n",
      "   1. S10.pkl\n",
      "   2. S11.pkl\n",
      "   3. S13.pkl\n",
      "   4. S14.pkl\n",
      "   5. S15.pkl\n",
      "   6. S16.pkl\n",
      "   7. S17.pkl\n",
      "   8. S2.pkl\n",
      "   9. S3.pkl\n",
      "  10. S4.pkl\n",
      "     ... and 5 more files\n",
      "\n",
      "🔍 Analyzing sample subject: S10.pkl\n",
      "\n",
      "📋 Top-level data structure:\n",
      "  - 'signal': <class 'dict'>\n",
      "  - 'label': <class 'numpy.ndarray'>\n",
      "  - 'subject': <class 'str'>\n",
      "\n",
      "📶 Signal modalities:\n",
      "  - 'chest': <class 'dict'>\n",
      "    Nested structure:\n",
      "      • ACC: (3847200, 3) (float64)\n",
      "      • ECG: (3847200, 1) (float64)\n",
      "      • EMG: (3847200, 1) (float64)\n",
      "      • EDA: (3847200, 1) (float64)\n",
      "      • Temp: (3847200, 1) (float32)\n",
      "      • Resp: (3847200, 1) (float64)\n",
      "  - 'wrist': <class 'dict'>\n",
      "    Nested structure:\n",
      "      • ACC: (175872, 3) (float64)\n",
      "      • BVP: (351744, 1) (float64)\n",
      "      • EDA: (21984, 1) (float64)\n",
      "      • TEMP: (21984, 1) (float64)\n",
      "\n",
      "🏷️ Labels:\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  Shape: (3847200,)\n",
      "  Data type: int32\n",
      "  Unique labels: [0 1 2 3 4 5 6 7]\n",
      "  Label meanings:\n",
      "    0: Not defined/Transient (1,589,000 samples, 41.3%)\n",
      "    1: Baseline (826,000 samples, 21.5%)\n",
      "    2: Stress (507,500 samples, 13.2%)\n",
      "    3: Amusement (260,400 samples, 6.8%)\n",
      "    4: Meditation (557,200 samples, 14.5%)\n"
     ]
    }
   ],
   "source": [
    "def explore_wesad_structure():\n",
    "    \"\"\"Explore the actual WESAD dataset structure (corrected version)\"\"\"\n",
    "    \n",
    "    wesad_path = os.path.join(project_root, \"data\", \"raw\", \"wesad\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WESAD DATASET STRUCTURE ANALYSIS (CORRECTED)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Find all pickle files\n",
    "    pickle_files = []\n",
    "    for root, dirs, files in os.walk(wesad_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.pkl'):\n",
    "                pickle_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"📁 Found {len(pickle_files)} subject files:\")\n",
    "    for i, pfile in enumerate(pickle_files[:10]):\n",
    "        print(f\"  {i+1:2d}. {os.path.basename(pfile)}\")\n",
    "    \n",
    "    if len(pickle_files) > 10:\n",
    "        print(f\"     ... and {len(pickle_files) - 10} more files\")\n",
    "    \n",
    "    # Load one subject to understand the actual data structure\n",
    "    print(f\"\\n🔍 Analyzing sample subject: {os.path.basename(pickle_files[0])}\")\n",
    "    \n",
    "    try:\n",
    "        with open(pickle_files[0], 'rb') as f:\n",
    "            subject_data = pickle.load(f, encoding='latin1')\n",
    "        \n",
    "        print(f\"\\n📋 Top-level data structure:\")\n",
    "        for key in subject_data.keys():\n",
    "            data_type = type(subject_data[key])\n",
    "            print(f\"  - '{key}': {data_type}\")\n",
    "        \n",
    "        # Explore 'signal' key more carefully\n",
    "        if 'signal' in subject_data:\n",
    "            signals = subject_data['signal']\n",
    "            print(f\"\\n📶 Signal modalities:\")\n",
    "            \n",
    "            for signal_key in signals.keys():\n",
    "                signal_data = signals[signal_key]\n",
    "                print(f\"  - '{signal_key}': {type(signal_data)}\")\n",
    "                \n",
    "                # Check if it's a dictionary (nested structure)\n",
    "                if isinstance(signal_data, dict):\n",
    "                    print(f\"    Nested structure:\")\n",
    "                    for nested_key in signal_data.keys():\n",
    "                        nested_data = signal_data[nested_key]\n",
    "                        if hasattr(nested_data, 'shape'):\n",
    "                            print(f\"      • {nested_key}: {nested_data.shape} ({nested_data.dtype})\")\n",
    "                        else:\n",
    "                            print(f\"      • {nested_key}: {type(nested_data)}\")\n",
    "                \n",
    "                # Check if it's a numpy array directly\n",
    "                elif hasattr(signal_data, 'shape'):\n",
    "                    print(f\"    Shape: {signal_data.shape}\")\n",
    "                    print(f\"    Data type: {signal_data.dtype}\")\n",
    "                    \n",
    "                    if signal_key == 'chest':\n",
    "                        print(f\"    🫀 Chest sensor channels:\")\n",
    "                        print(f\"       0: ECG - Electrocardiogram\")\n",
    "                        print(f\"       1: EDA - Electrodermal Activity\") \n",
    "                        print(f\"       2: EMG - Electromyogram\")\n",
    "                        print(f\"       3: Resp - Respiration\")\n",
    "                        print(f\"       4: Temp - Temperature\")\n",
    "        \n",
    "        # Explore 'label' key\n",
    "        if 'label' in subject_data:\n",
    "            labels = subject_data['label']\n",
    "            print(f\"\\n🏷️ Labels:\")\n",
    "            print(f\"  Type: {type(labels)}\")\n",
    "            \n",
    "            if hasattr(labels, 'shape'):\n",
    "                print(f\"  Shape: {labels.shape}\")\n",
    "                print(f\"  Data type: {labels.dtype}\")\n",
    "                unique_labels = np.unique(labels)\n",
    "                print(f\"  Unique labels: {unique_labels}\")\n",
    "                \n",
    "                label_meanings = {\n",
    "                    0: \"Not defined/Transient\",\n",
    "                    1: \"Baseline\", \n",
    "                    2: \"Stress\",\n",
    "                    3: \"Amusement\", \n",
    "                    4: \"Meditation\"\n",
    "                }\n",
    "                \n",
    "                print(f\"  Label meanings:\")\n",
    "                for label in unique_labels:\n",
    "                    if label <= 4:\n",
    "                        meaning = label_meanings.get(label, 'Unknown')\n",
    "                        count = np.sum(labels == label)\n",
    "                        percentage = (count / len(labels)) * 100\n",
    "                        print(f\"    {label}: {meaning} ({count:,} samples, {percentage:.1f}%)\")\n",
    "        \n",
    "        return pickle_files, subject_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading subject data: {e}\")\n",
    "        print(f\"   Let's try a different approach...\")\n",
    "        \n",
    "        # Alternative: try to load and inspect the raw data\n",
    "        try:\n",
    "            with open(pickle_files[0], 'rb') as f:\n",
    "                raw_data = pickle.load(f, encoding='latin1')\n",
    "            \n",
    "            print(f\"\\n🔧 Raw data inspection:\")\n",
    "            print(f\"   Type: {type(raw_data)}\")\n",
    "            \n",
    "            if isinstance(raw_data, dict):\n",
    "                print(f\"   Keys: {list(raw_data.keys())}\")\n",
    "                \n",
    "                # Try to navigate the structure manually\n",
    "                for key, value in raw_data.items():\n",
    "                    print(f\"   {key}: {type(value)}\")\n",
    "                    if isinstance(value, dict) and len(value) < 20:\n",
    "                        for subkey, subvalue in value.items():\n",
    "                            print(f\"     {subkey}: {type(subvalue)}\")\n",
    "                            if hasattr(subvalue, 'shape'):\n",
    "                                print(f\"       Shape: {subvalue.shape}\")\n",
    "            \n",
    "            return pickle_files, raw_data\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"❌ Alternative approach also failed: {e2}\")\n",
    "            return pickle_files, None\n",
    "\n",
    "# Explore dataset structure\n",
    "subject_files = explore_wesad_structure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
