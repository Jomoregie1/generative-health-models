{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e583d974",
   "metadata": {},
   "source": [
    "\n",
    "# TC‑MultiGAN — Training Notebook (Projection‑Style D, with Fixes)\n",
    "\n",
    "This notebook contains a **drop‑in training script** with the following fixes applied:\n",
    "\n",
    "- **Projection‑style conditioning in D**: replaces the auxiliary emotion head.\n",
    "- **Safe CE handling**: `ce_optional(...)` avoids crashes when the aux head is `None` (keeps code future‑proof).\n",
    "- **Feature‑matching fix**: handles the tuple return `(pooled, pooled_proj)` from `D.extract_features`.\n",
    "- **AntiAliasUp1D freeze**: ensures the fixed smoothing kernels are not trainable (safety freeze even if model file wasn’t edited).\n",
    "- **Config guard**: `lambda_tc` set to `0.0` (classification loss disabled when using projection‑style D).\n",
    "- **Sanity**: shape asserts use your configured `seq_length_low` / `seq_length_ecg`.\n",
    "\n",
    "> **Before you run**: make sure your project `src` folder is reachable (next cell shows how).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aeeed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using src path: C:\\Users\\Joseph\\generative-health-models\\src\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Project path setup (edit PROJECT_SRC if needed) ---\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to infer a 'src' folder by walking up parents\n",
    "nb_dir = Path.cwd()\n",
    "auto_src = None\n",
    "for p in [nb_dir, *nb_dir.parents]:\n",
    "    if (p / \"src\").exists():\n",
    "        auto_src = (p / \"src\").resolve()\n",
    "        break\n",
    "\n",
    "# If auto-detect fails, set this to your absolute path:\n",
    "# Example on Windows:\n",
    "# PROJECT_SRC = r\"C:\\Users\\Joseph\\generative-health-models\\src\"\n",
    "PROJECT_SRC = os.environ.get(\"PROJECT_SRC\", r\"C:\\Users\\Joseph\\generative-health-models\\src\")\n",
    "\n",
    "chosen = auto_src if auto_src and auto_src.exists() else Path(PROJECT_SRC)\n",
    "\n",
    "if not chosen.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find a 'src' folder at {chosen}.\\n\"\n",
    "        \"Set PROJECT_SRC to your absolute path or place this notebook inside the repo.\"\n",
    "    )\n",
    "\n",
    "if str(chosen) not in sys.path:\n",
    "    sys.path.insert(0, str(chosen))\n",
    "\n",
    "print(\"Using src path:\", chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "109ebe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.7.1+cu118 | CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Imports ---\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from evaluation.evaluator import run_epoch_evaluations\n",
    "from datasets.wesad import make_loader\n",
    "from models.tc_multigan import create_tc_multigan, boundary_loss\n",
    "from utils.config import _build_parser, _load_json_defaults\n",
    "from utils.config import parse_args\n",
    "\n",
    "\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e702cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  aug_jitter: 0.01\n",
      "  aug_scale: 0.1\n",
      "  batch_size: 16\n",
      "  boundary_margin_low: 0.92\n",
      "  ckpt_dir: results\\checkpoints\n",
      "  ckpt_interval: 5\n",
      "  condition_dim: 4\n",
      "  config_json: None\n",
      "  d_steps: 1\n",
      "  data_root: C:\\Users\\Joseph\\generative-health-models\\src\\train\\data\\processed\n",
      "  device: cuda\n",
      "  ecg_boundary_margin: 0.95\n",
      "  ecg_margin: 0.98\n",
      "  ema_decay: 0.999\n",
      "  epochs_ae: 20\n",
      "  epochs_gan: 50\n",
      "  fake_label: 0.2\n",
      "  fm_warmup_epochs: 15\n",
      "  fold: tc_multigan_fold_S10\n",
      "  fs_ecg: 175\n",
      "  fs_low: 4\n",
      "  g_steps: 2\n",
      "  gen_label: 0.8\n",
      "  hidden_dim: 256\n",
      "  inst_noise_std: 0.01\n",
      "  inst_noise_warm_epochs: 20\n",
      "  lambda_adv: 1.0\n",
      "  lambda_boundary_ecg: 0.08\n",
      "  lambda_boundary_low: 0.003\n",
      "  lambda_boundary_low_eda: None\n",
      "  lambda_boundary_low_resp: None\n",
      "  lambda_fm: 10.0\n",
      "  lambda_mismatch: 0.5\n",
      "  lambda_mm: 0.0\n",
      "  lambda_rec: 100.0\n",
      "  lambda_spec_ecg: 1.0\n",
      "  lambda_spec_low: 0.5\n",
      "  lambda_spike: 0.4\n",
      "  lambda_tc: 2.0\n",
      "  lambda_tv_ecg: 0.0\n",
      "  lambda_tv_eda: None\n",
      "  lambda_tv_low: 0.0\n",
      "  lambda_tv_resp: None\n",
      "  log_dir: results\\logs\n",
      "  lr_d: 0.0002\n",
      "  lr_g: 0.0001\n",
      "  n_channels: 3\n",
      "  num_workers: 4\n",
      "  r1_gamma: 1.0\n",
      "  real_label: 0.8\n",
      "  resume: \n",
      "  sample_dir: results\\samples\n",
      "  sample_interval: 1\n",
      "  sample_n: 8\n",
      "  seed: 42\n",
      "  seq_length: 240\n",
      "  seq_length_ecg: 5250\n",
      "  seq_length_low: 120\n",
      "  spec_ecg_fmax: 40.0\n",
      "  spec_ecg_fmin: 5.0\n",
      "  spec_eda_fmax: 0.25\n",
      "  spec_eda_fmin: 0.03\n",
      "  spec_low_fmax: None\n",
      "  spec_low_fmin: None\n",
      "  spec_nfft_ecg: 4096\n",
      "  spec_nfft_low: 256\n",
      "  spec_resp_fmax: 0.5\n",
      "  spec_resp_fmin: 0.1\n",
      "  spec_resp_shape_only: True\n",
      "  spec_warmup_epochs: 12\n",
      "  spike_tau: 2.0\n",
      "  train_split: train\n",
      "  use_ema: False\n",
      "  use_r1: False\n",
      "  val_split: test\n",
      "  weighted_sampling: False\n",
      "  z_dim: 128\n"
     ]
    }
   ],
   "source": [
    "@contextmanager\n",
    "def _clean_argv():\n",
    "    old = sys.argv\n",
    "    try:\n",
    "        sys.argv = [old[0]]  # keep program name only\n",
    "        yield\n",
    "    finally:\n",
    "        sys.argv = old\n",
    "\n",
    "# --- Find repo root automatically (fallback to your known path) ---\n",
    "def find_project_root(start: Path | None = None) -> Path:\n",
    "    start = Path(start or Path.cwd()).resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"src\").exists() and (p / \"data\" / \"processed\").exists():\n",
    "            return p\n",
    "    # fallback (edit if your path changes)\n",
    "    return Path(r\"C:\\Users\\Joseph\\generative-health-models\").resolve()\n",
    "\n",
    "with _clean_argv():\n",
    "    cfg = parse_args()  # safe in notebooks now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3dd98de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Anchor paths to the repo root ---\n",
    "project_root = find_project_root()\n",
    "cfg.data_root = str((project_root / \"data\" / \"processed\").resolve())\n",
    "cfg.ckpt_dir  = str((project_root / \"results\" / \"checkpoints\").resolve())\n",
    "cfg.sample_dir= str((project_root / \"results\" / \"samples\").resolve())\n",
    "cfg.log_dir   = str((project_root / \"results\" / \"logs\").resolve())\n",
    "\n",
    "# --- Pick the fold you want here ---\n",
    "cfg.fold = \"tc_multigan_fold_S10\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da006af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Utilities ---\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = float(decay)\n",
    "        self.shadow = {n: p.clone().detach()\n",
    "                       for n, p in model.named_parameters() if p.requires_grad}\n",
    "        self.backup = {}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad and n in self.shadow:\n",
    "                self.shadow[n].mul_(self.decay).add_(p.detach(), alpha=1.0 - self.decay)\n",
    "\n",
    "    def apply_to(self, model):\n",
    "        self.backup = {}\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad and n in self.shadow:\n",
    "                self.backup[n] = p.data.clone()\n",
    "                p.data.copy_(self.shadow[n].data)\n",
    "\n",
    "    def restore(self, model):\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad and n in self.backup:\n",
    "                p.data.copy_(self.backup[n])\n",
    "        self.backup = {}\n",
    "\n",
    "\n",
    "def _next_pow2(n: int) -> int:\n",
    "    return 1 << (n - 1).bit_length()\n",
    "\n",
    "\n",
    "def spectral_l1(x, y, fs, nfft=0, fmin=None, fmax=None, shape_only=False, eps=1e-8):\n",
    "    \"\"\"\n",
    "    x,y: (B,T) waveforms. fs in Hz.\n",
    "    shape_only=True: compare normalized spectral shapes (sum=1), not raw magnitudes.\n",
    "    \"\"\"\n",
    "    B, T = x.shape\n",
    "    use_nfft = _next_pow2(T) if (nfft is None or nfft == 0) else max(nfft, T)\n",
    "\n",
    "    X = torch.fft.rfft(x, n=use_nfft, dim=1)\n",
    "    Y = torch.fft.rfft(y, n=use_nfft, dim=1)\n",
    "    magX = X.abs()\n",
    "    magY = Y.abs()\n",
    "\n",
    "    if (fmin is not None) or (fmax is not None):\n",
    "        freqs = torch.fft.rfftfreq(use_nfft, d=1.0/fs).to(x.device)\n",
    "        mask = torch.ones_like(freqs, dtype=torch.bool)\n",
    "        if fmin is not None: mask &= (freqs >= float(fmin))\n",
    "        if fmax is not None: mask &= (freqs <= float(fmax))\n",
    "        magX = magX[:, mask]\n",
    "        magY = magY[:, mask]\n",
    "\n",
    "    if shape_only:\n",
    "        magX = magX / (magX.sum(dim=1, keepdim=True) + eps)\n",
    "        magY = magY / (magY.sum(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    return F.l1_loss(torch.log1p(magX), torch.log1p(magY))\n",
    "\n",
    "\n",
    "def set_requires_grad(net, flag: bool):\n",
    "    for p in net.parameters():\n",
    "        p.requires_grad_(flag)\n",
    "\n",
    "\n",
    "def d_hinge(real_logits, fake_logits):\n",
    "    # real wants > 1, fake wants < -1\n",
    "    loss_real = F.relu(1.0 - real_logits).mean()\n",
    "    loss_fake = F.relu(1.0 + fake_logits).mean()\n",
    "    return loss_real + loss_fake\n",
    "\n",
    "\n",
    "def g_hinge(fake_logits):\n",
    "    # generator wants logits >> 1\n",
    "    return -fake_logits.mean()\n",
    "\n",
    "\n",
    "def ce_optional(logits, y, criterion, device):\n",
    "    \"\"\"Cross entropy that safely returns 0 if logits or y are None.\"\"\"\n",
    "    if (logits is None) or (y is None):\n",
    "        return torch.zeros((), device=device)\n",
    "    return criterion(logits, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b59569",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Sampling ---\n",
    "\n",
    "def generate_and_save_samples(G, noise, cond_low, cfg, epoch, n_plot=4, save_denorm=True):\n",
    "    outdir = Path(cfg.sample_dir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        fake_low, fake_ecg = G(noise, cond_low)\n",
    "        fake_low_np = fake_low.detach().cpu().float().numpy()\n",
    "        fake_ecg_np = fake_ecg.detach().cpu().float().numpy()\n",
    "\n",
    "    # -------- shape checks & safe slicing --------\n",
    "    print(f\"[sample dbg] fake_low={fake_low_np.shape}  fake_ecg={fake_ecg_np.shape}  cond={tuple(cond_low.shape)}\")\n",
    "    if fake_low_np.ndim != 3:\n",
    "        raise ValueError(f\"[bad shape] fake_low ndim={fake_low_np.ndim}, expected 3 (N,T,C)\")\n",
    "    N, T_low, C_low = fake_low_np.shape\n",
    "    if T_low != cfg.seq_length_low:\n",
    "        print(f\"[warn] fake_low T={T_low}, expected {cfg.seq_length_low}\")\n",
    "    if C_low < 2:\n",
    "        raise ValueError(f\"[bad shape] fake_low has {C_low} channel(s); need ≥2 (EDA, RESP)\")\n",
    "    if C_low > 2:\n",
    "        print(f\"[warn] fake_low has {C_low} channels; plotting ONLY the first two (EDA/RESP).\" )\n",
    "    low_plot = fake_low_np[:, :, :2]\n",
    "\n",
    "    if fake_ecg_np.ndim == 2:\n",
    "        fake_ecg_np = fake_ecg_np[..., None]  # (N,T)->(N,T,1)\n",
    "    elif fake_ecg_np.ndim != 3:\n",
    "        raise ValueError(f\"[bad shape] fake_ecg ndim={fake_ecg_np.ndim}, expected 3\")\n",
    "    if fake_ecg_np.shape[-1] < 1:\n",
    "        raise ValueError(\"[bad shape] fake_ecg has 0 channels\")\n",
    "    ecg_plot = fake_ecg_np[:, :, 0:1]\n",
    "\n",
    "    # -------- save normalized arrays --------\n",
    "    np.save(outdir / f\"fake_low_epoch_{epoch:03d}.npy\", fake_low_np)\n",
    "    np.save(outdir / f\"fake_ecg_epoch_{epoch:03d}.npy\", fake_ecg_np)\n",
    "\n",
    "    # -------- optional: denormalize for visualization --------\n",
    "    low_denorm = None\n",
    "    ecg_denorm = None\n",
    "    if save_denorm:\n",
    "        try:\n",
    "            fold_dir = Path(cfg.data_root) / cfg.fold\n",
    "            st_low = np.load(fold_dir / \"norm_low.npz\")\n",
    "            st_ecg = np.load(fold_dir / \"norm_ecg.npz\")\n",
    "            mu_low, sd_low = st_low[\"mean\"].astype(np.float32), st_low[\"std\"].astype(np.float32)   # (2,), (2,)\n",
    "            mu_ecg, sd_ecg = st_ecg[\"mean\"].astype(np.float32), st_ecg[\"std\"].astype(np.float32)   # (1,), (1,)\n",
    "\n",
    "            # broadcast only up to available stats\n",
    "            k_low = min(fake_low_np.shape[-1], mu_low.shape[0])\n",
    "            k_ecg = min(fake_ecg_np.shape[-1], mu_ecg.shape[0])\n",
    "\n",
    "            low_denorm = fake_low_np.copy()\n",
    "            low_denorm[..., :k_low] = low_denorm[..., :k_low] * sd_low[:k_low][None, None, :] + mu_low[:k_low][None, None, :]\n",
    "\n",
    "            ecg_denorm = fake_ecg_np.copy()\n",
    "            ecg_denorm[..., :k_ecg] = ecg_denorm[..., :k_ecg] * sd_ecg[:k_ecg][None, None, :] + mu_ecg[:k_ecg][None, None, :]\n",
    "\n",
    "            np.save(outdir / f\"fake_low_epoch_{epoch:03d}_DENORM.npy\", low_denorm)\n",
    "            np.save(outdir / f\"fake_ecg_epoch_{epoch:03d}_DENORM.npy\", ecg_denorm)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] denorm skipped: {e}\")\n",
    "\n",
    "    # -------- quick plots (normalized) --------\n",
    "    n = int(min(n_plot, low_plot.shape[0], ecg_plot.shape[0]))\n",
    "    for i in range(n):\n",
    "        # EDA/RESP\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(8, 4), sharex=True)\n",
    "        axes[0].plot(low_plot[i, :, 0]); axes[0].set_title(f\"Low-rate (EDA) — sample {i}\")\n",
    "        axes[1].plot(low_plot[i, :, 1]); axes[1].set_title(\"Low-rate (RESP)\")\n",
    "        axes[1].set_xlabel(\"Time (4 Hz steps)\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(outdir / f\"fake_low_{epoch:03d}_sample_{i}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        # ECG\n",
    "        fig2, ax2 = plt.subplots(1, 1, figsize=(8, 2.5))\n",
    "        ax2.plot(ecg_plot[i, :, 0]); ax2.set_title(f\"ECG — sample {i}\")\n",
    "        ax2.set_xlabel(\"Time (175 Hz steps)\")\n",
    "        fig2.tight_layout()\n",
    "        fig2.savefig(outdir / f\"fake_ecg_{epoch:03d}_sample_{i}.png\")\n",
    "        plt.close(fig2)\n",
    "\n",
    "        # Optional denorm plots for eyeballing (if available)\n",
    "        if save_denorm and (low_denorm is not None) and (ecg_denorm is not None):\n",
    "            fig3, axes3 = plt.subplots(2, 1, figsize=(8, 4), sharex=True)\n",
    "            axes3[0].plot(low_denorm[i, :, 0]); axes3[0].set_title(f\"[DENORM] EDA — sample {i}\")\n",
    "            if low_denorm.shape[-1] > 1:\n",
    "                axes3[1].plot(low_denorm[i, :, 1]); axes3[1].set_title(\"[DENORM] RESP\")\n",
    "            axes3[1].set_xlabel(\"Time (4 Hz steps)\")\n",
    "            fig3.tight_layout()\n",
    "            fig3.savefig(outdir / f\"fake_low_{epoch:03d}_sample_{i}_DENORM.png\")\n",
    "            plt.close(fig3)\n",
    "\n",
    "            fig4, ax4 = plt.subplots(1, 1, figsize=(8, 2.5))\n",
    "            ax4.plot(ecg_denorm[i, :, 0]); ax4.set_title(f\"[DENORM] ECG — sample {i}\")\n",
    "            ax4.set_xlabel(\"Time (175 Hz steps)\")\n",
    "            fig4.tight_layout()\n",
    "            fig4.savefig(outdir / f\"fake_ecg_{epoch:03d}_sample_{i}_DENORM.png\")\n",
    "            plt.close(fig4)\n",
    "\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f336771",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- One training epoch (with projection D fixes) ---\n",
    "def train_one_epoch(\n",
    "    G, D, data_loader, opt_g, opt_d, device, cfg,\n",
    "    criterion_aux, epoch, ema=None, use_ema=False\n",
    "):\n",
    "    G.train(); D.train()\n",
    "    total_loss_g = 0.0\n",
    "    total_loss_d = 0.0\n",
    "\n",
    "    # ----- epoch-scheduled instance noise -----\n",
    "    noise0 = float(getattr(cfg, \"inst_noise_std\", 0.0))\n",
    "    warm   = int(getattr(cfg, \"inst_noise_warm_epochs\", 0))\n",
    "    noise_std = float(noise0 * max(0.0, 1.0 - (epoch - 1) / max(1, warm)))\n",
    "    def add_noise(x):\n",
    "        return x + noise_std * torch.randn_like(x) if noise_std > 0 else x\n",
    "\n",
    "    # ----- feature-matching warmup ramp -----\n",
    "    fm_base = float(getattr(cfg, \"lambda_fm\", 0.0))\n",
    "    fm_warm = int(getattr(cfg, \"fm_warmup_epochs\", 0))\n",
    "    ramp = min(1.0, max(0.0, epoch / fm_warm)) if fm_warm > 0 else 1.0\n",
    "    fm_w_curr = fm_base * ramp\n",
    "\n",
    "    # ----- spectral warmup (ECG) -----\n",
    "    spec_warm = int(getattr(cfg, \"spec_warmup_epochs\", 12))\n",
    "    spec_ramp = min(1.0, max(0.0, (epoch - 1) / float(max(1, spec_warm))))\n",
    "    lambda_spec_ecg_eff = float(cfg.lambda_spec_ecg) * spec_ramp\n",
    "\n",
    "    # ---- symmetric observation margins ----\n",
    "    margin_low = float(getattr(cfg, \"boundary_margin_low\", 0.92))\n",
    "    ecg_margin = float(getattr(cfg, \"ecg_margin\", 0.98))  # clamp used for the D (OBS view)\n",
    "    # Boundary penalty can use the same or a slightly tighter margin; both tunable via cfg\n",
    "    ecg_boundary_margin = float(getattr(cfg, \"ecg_boundary_margin\", ecg_margin))\n",
    "    lambda_boundary_ecg = float(getattr(cfg, \"lambda_boundary_ecg\", 0.0))\n",
    "\n",
    "    # ---- steps schedule ----\n",
    "    g_steps = int(getattr(cfg, \"g_steps\", 1))\n",
    "    d_steps = int(getattr(cfg, \"d_steps\", 1))\n",
    "    if epoch <= 15:\n",
    "        g_steps, d_steps = 1, 2\n",
    "    else:\n",
    "        g_steps, d_steps = 1, 1\n",
    "\n",
    "    did_label_check = False\n",
    "\n",
    "    for batch in data_loader:\n",
    "        # --------- unpack ---------\n",
    "        sig_low  = batch[\"signal_low\"].to(device).float()   # (B, T_low, 2)\n",
    "        sig_ecg  = batch[\"signal_ecg\"].to(device).float()   # (B, T_ecg, 1)\n",
    "        cond_low = batch[\"condition\"].to(device).float()    # (B, T_low, K)\n",
    "\n",
    "        labels = batch.get(\"label\")\n",
    "        if labels is not None:\n",
    "            labels = labels.to(device)\n",
    "            y = labels if labels.ndim == 1 else labels.argmax(dim=-1)\n",
    "            y = y.long()\n",
    "            if y.min() >= 1 and y.max() == cfg.condition_dim:\n",
    "                y = y - 1\n",
    "            if not did_label_check:\n",
    "                if y.min() < 0 or y.max() >= cfg.condition_dim:\n",
    "                    raise ValueError(f\"Label indices out of range: {y.min().item()}..{y.max().item()} (K={cfg.condition_dim})\")\n",
    "                did_label_check = True\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        # ===================== D STEP(S) =====================\n",
    "        set_requires_grad(D, True)\n",
    "        opt_d.zero_grad()\n",
    "        batch_loss_d = 0.0\n",
    "\n",
    "        for _ in range(d_steps):\n",
    "            # --- REAL (clamp -> noise -> D) ---\n",
    "            real_low_obs = sig_low.clamp(-margin_low, margin_low)\n",
    "            real_ecg_obs = sig_ecg.clamp(-ecg_margin, ecg_margin)\n",
    "\n",
    "            real_low_in = add_noise(real_low_obs).detach().requires_grad_(True)\n",
    "            real_ecg_in = add_noise(real_ecg_obs).detach().requires_grad_(True)\n",
    "\n",
    "            real_logits, real_aux, _ = D(real_low_in, real_ecg_in, cond_low)\n",
    "            real_adv_loss = torch.relu(1.0 - real_logits).mean()  # hinge real\n",
    "            real_aux_loss = ce_optional(real_aux, y, criterion_aux, device)\n",
    "\n",
    "            # --- R1 on observed inputs to D ---\n",
    "            r1 = torch.zeros((), device=device)\n",
    "            if getattr(cfg, \"use_r1\", False):\n",
    "                grads = torch.autograd.grad(real_logits.sum(),\n",
    "                                            [real_low_in, real_ecg_in],\n",
    "                                            create_graph=True, retain_graph=True, only_inputs=True)\n",
    "                r1 = sum(g.reshape(g.size(0), -1).pow(2).sum(dim=1).mean() for g in grads)\n",
    "\n",
    "            # --- FAKE for D (clamp -> noise -> D) ---\n",
    "            with torch.no_grad():\n",
    "                z_d = torch.randn(sig_low.size(0), cfg.z_dim, device=device)\n",
    "                fake_low_d, fake_ecg_d = G(z_d, cond_low)\n",
    "\n",
    "            fake_low_obs_d = fake_low_d.clamp(-margin_low, margin_low)\n",
    "            fake_ecg_obs_d = fake_ecg_d.clamp(-ecg_margin, ecg_margin)\n",
    "\n",
    "            fake_logits_d, fake_aux_d, _ = D(add_noise(fake_low_obs_d), add_noise(fake_ecg_obs_d), cond_low)\n",
    "            fake_adv_loss = torch.relu(1.0 + fake_logits_d).mean()  # hinge fake\n",
    "            fake_aux_loss = ce_optional(fake_aux_d, y, criterion_aux, device)\n",
    "\n",
    "            # --- mismatch (real signals, wrong condition) on observed view ---\n",
    "            mismatch_w = float(getattr(cfg, \"lambda_mismatch\", 0.5))\n",
    "            B = sig_low.size(0)\n",
    "            perm = torch.randperm(B, device=device)\n",
    "            if B > 1 and torch.all(perm == torch.arange(B, device=device)):\n",
    "                perm = torch.roll(perm, 1)\n",
    "            cond_wrong = cond_low[perm]\n",
    "            wrong_logits, _, _ = D(add_noise(real_low_obs.detach()), add_noise(real_ecg_obs.detach()), cond_wrong)\n",
    "            wrong_adv_loss = torch.relu(1.0 + wrong_logits).mean()\n",
    "\n",
    "            # --- total D loss ---\n",
    "            loss_d = (\n",
    "                cfg.lambda_adv * (real_adv_loss + fake_adv_loss + mismatch_w * wrong_adv_loss)\n",
    "                + cfg.lambda_tc * (real_aux_loss + fake_aux_loss)\n",
    "                + 0.5 * float(getattr(cfg, \"r1_gamma\", 0.0)) * r1\n",
    "            )\n",
    "            loss_d.backward()\n",
    "            batch_loss_d += loss_d.item()\n",
    "\n",
    "        opt_d.step()\n",
    "        total_loss_d += batch_loss_d / d_steps\n",
    "\n",
    "        # ===================== G STEP(S) =====================\n",
    "        set_requires_grad(D, False)\n",
    "        opt_g.zero_grad()\n",
    "        batch_loss_g = 0.0\n",
    "\n",
    "        for _ in range(g_steps):\n",
    "            z_g = torch.randn(sig_low.size(0), cfg.z_dim, device=device)\n",
    "            fake_low_g, fake_ecg_g = G(z_g, cond_low)\n",
    "\n",
    "            # Observed (clamped) views for D/FM/spec\n",
    "            real_low_obs = sig_low.clamp(-margin_low, margin_low)\n",
    "            real_ecg_obs = sig_ecg.clamp(-ecg_margin, ecg_margin)\n",
    "            fake_low_obs = fake_low_g.clamp(-margin_low, margin_low)\n",
    "            fake_ecg_obs = fake_ecg_g.clamp(-ecg_margin, ecg_margin)\n",
    "\n",
    "            gen_logits, gen_aux, fake_feat = D(fake_low_obs, fake_ecg_obs, cond_low)\n",
    "            adv_g = -gen_logits.mean()  # hinge generator\n",
    "            gen_aux_loss = ce_optional(gen_aux, y, criterion_aux, device)\n",
    "\n",
    "            # ----- Feature Matching on observed view -----\n",
    "            fm_loss = torch.zeros((), device=device)\n",
    "            if fm_w_curr > 0.0:\n",
    "                with torch.no_grad():\n",
    "                    rf_low = add_noise(real_low_obs)\n",
    "                    rf_ecg = add_noise(real_ecg_obs)\n",
    "                    real_feat = D.extract_features(rf_low, rf_ecg, cond_low)\n",
    "                    if isinstance(real_feat, tuple):\n",
    "                        real_feat = real_feat[0]\n",
    "                if isinstance(fake_feat, tuple):\n",
    "                    fake_feat = fake_feat[0]\n",
    "                fm_loss = F.l1_loss(fake_feat.mean(dim=0), real_feat.mean(dim=0))\n",
    "\n",
    "            # ----- Moment Matching (tanh-aware on observed view) -----\n",
    "            lam_mm_low = float(getattr(cfg, \"lambda_mm\", 0.0))\n",
    "            lam_mm_ecg = float(getattr(cfg, \"lambda_mm_ecg\", 0.0))  # optional; default 0.0\n",
    "\n",
    "            mm_low = torch.zeros((), device=device)\n",
    "            mm_ecg = torch.zeros((), device=device)\n",
    "\n",
    "            if lam_mm_low > 0.0:\n",
    "                def mm_stats_low(x):\n",
    "                    x = x.clamp(-margin_low, margin_low)   # same observed view as D/spec\n",
    "                    mu = x.mean(dim=(0,1))                 # per-channel (EDA, RESP)\n",
    "                    sd = x.std (dim=(0,1))\n",
    "                    return mu, sd\n",
    "\n",
    "                mu_r_low, sd_r_low = mm_stats_low(sig_low)\n",
    "                mu_f_low, sd_f_low = mm_stats_low(fake_low_g)\n",
    "                mm_low = F.l1_loss(mu_f_low, mu_r_low) + F.l1_loss(sd_f_low, sd_r_low)\n",
    "\n",
    "            if lam_mm_ecg > 0.0:\n",
    "                def mm_stats_ecg(x):                       # x: (B,T,1)\n",
    "                    x = x.clamp(-ecg_margin, ecg_margin)   # observed view\n",
    "                    return x.mean(), x.std()\n",
    "                mu_re, sd_re = mm_stats_ecg(sig_ecg)\n",
    "                mu_fe, sd_fe = mm_stats_ecg(fake_ecg_g)\n",
    "                mm_ecg = F.l1_loss(mu_fe, mu_re) + F.l1_loss(sd_fe, sd_re)\n",
    "\n",
    "            # ----- temporal TV penalties -----\n",
    "            tv_w_low_default = float(getattr(cfg, \"lambda_tv_low\", 0.0))\n",
    "            tv_eda  = (fake_low_g[:, 1:, 0] - fake_low_g[:, :-1, 0]).abs().mean()\n",
    "            tv_resp = (fake_low_g[:, 1:, 1] - fake_low_g[:, :-1, 1]).abs().mean()\n",
    "            tv_w_eda  = float(getattr(cfg, \"lambda_tv_eda\",  tv_w_low_default or 0.0) or tv_w_low_default)\n",
    "            tv_w_resp = float(getattr(cfg, \"lambda_tv_resp\", tv_w_low_default or 0.0) or tv_w_low_default)\n",
    "            tv_low = tv_w_eda * tv_eda + tv_w_resp * tv_resp\n",
    "\n",
    "            tv_ecg = (fake_ecg_g[:, 1:, :] - fake_ecg_g[:, :-1, :]).abs().mean()\n",
    "\n",
    "            # ----- Spike penalty (hinged TV on Δ) -----\n",
    "            tau = float(getattr(cfg, \"spike_tau\", 2.0))\n",
    "            d_ecg = fake_ecg_g[:, 1:, :] - fake_ecg_g[:, :-1, :]\n",
    "            spike_loss = torch.relu(d_ecg.abs() - tau).pow(2).mean()\n",
    "\n",
    "            # ---- ECG boundary penalty on RAW generator output (not the clamped view) ----\n",
    "            boundary_pen_ecg = lambda_boundary_ecg * boundary_loss(fake_ecg_g, ecg_boundary_margin)\n",
    "\n",
    "            # ---- Spectral losses on observed view (shape to (B, T)) ----\n",
    "            real_low_eda  = real_low_obs[..., 0]\n",
    "            real_low_resp = real_low_obs[..., 1]\n",
    "            fake_low_eda  = fake_low_obs[..., 0]\n",
    "            fake_low_resp = fake_low_obs[..., 1]\n",
    "\n",
    "            spec_low_eda  = spectral_l1(fake_low_eda,  real_low_eda,  fs=cfg.fs_low,\n",
    "                                        nfft=cfg.spec_nfft_low,\n",
    "                                        fmin=getattr(cfg, \"spec_eda_fmin\", None),\n",
    "                                        fmax=getattr(cfg, \"spec_eda_fmax\", None))\n",
    "            spec_low_resp = spectral_l1(fake_low_resp, real_low_resp, fs=cfg.fs_low,\n",
    "                                        nfft=cfg.spec_nfft_low,\n",
    "                                        fmin=getattr(cfg, \"spec_resp_fmin\", None),\n",
    "                                        fmax=getattr(cfg, \"spec_resp_fmax\", None),\n",
    "                                        shape_only=bool(getattr(cfg, \"spec_resp_shape_only\", True)))\n",
    "            spec_low = 0.5 * spec_low_eda + 0.5 * spec_low_resp\n",
    "\n",
    "            real_ecg_1c = real_ecg_obs.squeeze(-1)\n",
    "            fake_ecg_1c = fake_ecg_obs.squeeze(-1)\n",
    "            spec_ecg = spectral_l1(fake_ecg_1c, real_ecg_1c, fs=cfg.fs_ecg,\n",
    "                                   nfft=cfg.spec_nfft_ecg,\n",
    "                                   fmin=cfg.spec_ecg_fmin, fmax=cfg.spec_ecg_fmax)\n",
    "\n",
    "            # ---- Boundary penalty for LOW tanh head (on raw generator output) ----\n",
    "            low_unit = fake_low_g  # generator output\n",
    "            margin = margin_low\n",
    "            pen_eda  = torch.relu(low_unit[..., 0].abs() - margin).pow(2).mean()\n",
    "            pen_resp = torch.relu(low_unit[..., 1].abs() - margin).pow(2).mean()\n",
    "            lam_b_base  = float(getattr(cfg, \"lambda_boundary_low\", 0.0))\n",
    "            lam_b_eda   = float(getattr(cfg, \"lambda_boundary_low_eda\",  None) or lam_b_base)\n",
    "            lam_b_resp  = float(getattr(cfg, \"lambda_boundary_low_resp\", None) or lam_b_base)\n",
    "            boundary_pen = lam_b_eda * pen_eda + lam_b_resp * pen_resp\n",
    "\n",
    "            # ----- final generator loss -----\n",
    "            loss_g = (\n",
    "                cfg.lambda_adv * adv_g\n",
    "                + cfg.lambda_tc * gen_aux_loss\n",
    "                + fm_w_curr * fm_loss\n",
    "                + tv_low\n",
    "                + float(cfg.lambda_tv_ecg) * tv_ecg\n",
    "                + float(cfg.lambda_spec_low) * spec_low\n",
    "                + float(lambda_spec_ecg_eff) * spec_ecg\n",
    "                + float(getattr(cfg, \"lambda_spike\", 0.0)) * spike_loss\n",
    "                + boundary_pen\n",
    "                + boundary_pen_ecg\n",
    "                + lam_mm_low * mm_low\n",
    "                + lam_mm_ecg * mm_ecg\n",
    "            )\n",
    "\n",
    "            loss_g.backward()\n",
    "            opt_g.step()\n",
    "            if use_ema and (ema is not None):\n",
    "                ema.update(G)\n",
    "            batch_loss_g += loss_g.item()\n",
    "\n",
    "        total_loss_g += batch_loss_g / g_steps\n",
    "\n",
    "    return total_loss_g / len(data_loader), total_loss_d / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0145d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Validation (hinge + optional FM) ---\n",
    "def validate(G, D, data_loader, device, cfg, criterion_aux):\n",
    "    G.eval(); D.eval()\n",
    "    val_loss_g = 0.0; val_loss_d = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            sig_low  = batch[\"signal_low\"].to(device).float()\n",
    "            sig_ecg  = batch[\"signal_ecg\"].to(device).float()\n",
    "            cond_low = batch[\"condition\"].to(device).float()\n",
    "\n",
    "            labels = batch.get(\"label\")\n",
    "            y = None\n",
    "            if labels is not None:\n",
    "                y = (labels.to(device).long()\n",
    "                     if labels.ndim == 1 else labels.to(device).argmax(dim=-1).long())\n",
    "                if y.min() >= 1 and y.max() == cfg.condition_dim:\n",
    "                    y = y - 1\n",
    "\n",
    "            # --- symmetric observation margins (same as train) ---\n",
    "            margin_low = float(getattr(cfg, \"boundary_margin_low\", 0.92))\n",
    "            ecg_margin = float(getattr(cfg, \"ecg_margin\", 0.98))\n",
    "\n",
    "            # Clamp real before the discriminator / metrics\n",
    "            real_low_obs = sig_low.clamp(-margin_low,  margin_low)\n",
    "            real_ecg_obs = sig_ecg.clamp(-ecg_margin,   ecg_margin)\n",
    "\n",
    "            # Make fake, then clamp before the discriminator / metrics\n",
    "            z = torch.randn(sig_low.size(0), cfg.z_dim, device=device)\n",
    "            fake_low, fake_ecg = G(z, cond_low)\n",
    "            fake_low_obs = fake_low.clamp(-margin_low, margin_low)\n",
    "            fake_ecg_obs = fake_ecg.clamp(-ecg_margin,  ecg_margin)\n",
    "\n",
    "            # -------------------- D loss (hinge, no R1 in val) --------------------\n",
    "            real_logits, real_aux, _ = D(real_low_obs, real_ecg_obs, cond_low)\n",
    "            fake_logits, fake_aux, _ = D(fake_low_obs, fake_ecg_obs, cond_low)\n",
    "\n",
    "            adv_d = d_hinge(real_logits, fake_logits)\n",
    "            aux_d = ce_optional(real_aux, y, criterion_aux, device) + ce_optional(fake_aux, y, criterion_aux, device)\n",
    "            loss_d = cfg.lambda_adv * adv_d + cfg.lambda_tc * aux_d\n",
    "            val_loss_d += loss_d.item()\n",
    "\n",
    "            # -------------------- G loss (hinge + FM) --------------------\n",
    "            gen_logits, gen_aux, fake_feat = D(fake_low_obs, fake_ecg_obs, cond_low)\n",
    "            adv_g = g_hinge(gen_logits)\n",
    "            aux_g = ce_optional(gen_aux, y, criterion_aux, device)\n",
    "\n",
    "            # Feature matching uses clamped real features\n",
    "            if float(getattr(cfg, \"lambda_fm\", 0.0)) > 0.0:\n",
    "                real_feat = D.extract_features(real_low_obs, real_ecg_obs, cond_low)\n",
    "                if isinstance(real_feat, tuple):\n",
    "                    real_feat = real_feat[0]\n",
    "                if isinstance(fake_feat, tuple):\n",
    "                    fake_feat = fake_feat[0]\n",
    "                fm_loss = F.l1_loss(fake_feat.mean(dim=0), real_feat.mean(dim=0))\n",
    "            else:\n",
    "                fm_loss = torch.zeros((), device=device)\n",
    "\n",
    "            loss_g = cfg.lambda_adv * adv_g + cfg.lambda_tc * aux_g + float(getattr(cfg, \"lambda_fm\", 0.0)) * fm_loss\n",
    "            val_loss_g += loss_g.item()\n",
    "\n",
    "    n = len(data_loader)\n",
    "    return val_loss_g / n, val_loss_d / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_dir = Path(cfg.data_root) / cfg.fold\n",
    "fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Using fold_dir:\", fold_dir)\n",
    "\n",
    "required = [\"train_X_low.npy\", \"train_X_ecg.npy\", \"train_m1_seq.npy\"]\n",
    "missing = [f for f in required if not (fold_dir / f).exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing in {fold_dir}: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load RAW train arrays (the same files your dataset uses)\n",
    "# X_low = np.load(fold_dir / f\"{cfg.train_split}_X_low.npy\").astype(np.float32)   # (N, T_low, 2) [EDA, RESP]\n",
    "# X_ecg = np.load(fold_dir / f\"{cfg.train_split}_X_ecg.npy\").astype(np.float32)   # (N, T_ecg, 1)\n",
    "\n",
    "# # Compute per-channel mean/std across N and T\n",
    "# mu_low = X_low.reshape(-1, X_low.shape[-1]).mean(axis=0).astype(np.float32)  # (2,)\n",
    "# sd_low = X_low.reshape(-1, X_low.shape[-1]).std (axis=0).astype(np.float32)  # (2,)\n",
    "# mu_ecg = X_ecg.reshape(-1, X_ecg.shape[-1]).mean(axis=0).astype(np.float32)  # (1,)\n",
    "# sd_ecg = X_ecg.reshape(-1, X_ecg.shape[-1]).std (axis=0).astype(np.float32)  # (1,)\n",
    "\n",
    "# # Optional: print to review\n",
    "# print(\"Train stats (LOW)  mean:\", mu_low.tolist(), \"std:\", sd_low.tolist())\n",
    "# print(\"Train stats (ECG)  mean:\", mu_ecg.tolist(), \"std:\", sd_ecg.tolist())\n",
    "\n",
    "# # Save in the exact files your code expects for de-normalization\n",
    "# np.savez(fold_dir / \"norm_low.npz\", mean=mu_low, std=sd_low, channels=np.array([\"EDA\",\"RESP\"]))\n",
    "# np.savez(fold_dir / \"norm_ecg.npz\", mean=mu_ecg, std=sd_ecg, channels=np.array([\"ECG\"]))\n",
    "# print(\"Saved:\", fold_dir / \"norm_low.npz\", \"and\", fold_dir / \"norm_ecg.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1865e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Data loaders ---\n",
    "device = torch.device(cfg.device)\n",
    "# --- after saving or editing norm_*.npz ---\n",
    "for name in [\"train_loader\", \"val_loader\"]:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "import gc; gc.collect()\n",
    "\n",
    "train_loader = make_loader(\n",
    "    root_dir=cfg.data_root, fold=cfg.fold, split=cfg.train_split,\n",
    "    window_size_low=cfg.seq_length_low, batch_size=cfg.batch_size,\n",
    "    shuffle=True, num_workers=cfg.num_workers, weighted_sampling=cfg.weighted_sampling,\n",
    "    condition_dim=cfg.condition_dim,\n",
    "    augment=True,\n",
    "    normalize=True, normalize_ecg=True,\n",
    "    stats_low_path=str(fold_dir / \"norm_low.npz\"),\n",
    "    stats_ecg_path=str(fold_dir / \"norm_ecg.npz\"),\n",
    "    expected_ecg_len=cfg.seq_length_ecg,\n",
    "    force_use_stats=True,\n",
    "    use_split_stats_if_needed=False,\n",
    ")\n",
    "\n",
    "val_loader = make_loader(\n",
    "    root_dir=cfg.data_root, fold=cfg.fold, split=cfg.val_split,\n",
    "    window_size_low=cfg.seq_length_low, batch_size=cfg.batch_size,\n",
    "    shuffle=False, num_workers=cfg.num_workers, condition_dim=cfg.condition_dim,\n",
    "    augment=False,\n",
    "    normalize=True, normalize_ecg=True,\n",
    "    stats_low_path=str(fold_dir / \"norm_low.npz\"),\n",
    "    stats_ecg_path=str(fold_dir / \"norm_ecg.npz\"),\n",
    "    expected_ecg_len=cfg.seq_length_ecg,\n",
    "    force_use_stats=True,\n",
    "    use_split_stats_if_needed=False,\n",
    ")\n",
    "\n",
    "print(\"Train batches:\", len(train_loader), \"Val batches:\", len(val_loader))\n",
    "\n",
    "# Quick sanity: shapes should match your cfg lengths\n",
    "b = next(iter(val_loader))\n",
    "assert b[\"signal_low\"].shape[1] == cfg.seq_length_low and b[\"signal_ecg\"].shape[1] == cfg.seq_length_ecg, (\n",
    "    f\"Expected low/ecg lengths {cfg.seq_length_low}/{cfg.seq_length_ecg}; got \"\n",
    "    f\"{b['signal_low'].shape[1]}/{b['signal_ecg'].shape[1]}\"\n",
    ")\n",
    "print(\"[sanity] val low mean/std:\", b[\"signal_low\"].mean().item(), b[\"signal_low\"].std().item())\n",
    "print(\"[sanity] val ecg mean/std:\", b[\"signal_ecg\"].mean().item(),  b[\"signal_ecg\"].std().item())\n",
    "del b\n",
    "\n",
    "# Global val stats (optional)\n",
    "sum_low = sumsq_low = count_low = 0.0\n",
    "sum_ecg = sumsq_ecg = count_ecg = 0.0\n",
    "for batch in val_loader:\n",
    "    x_low = batch[\"signal_low\"].double()\n",
    "    x_ecg = batch[\"signal_ecg\"].double()\n",
    "    sum_low   += x_low.sum().item()\n",
    "    sumsq_low += (x_low * x_low).sum().item()\n",
    "    count_low += x_low.numel()\n",
    "    sum_ecg   += x_ecg.sum().item()\n",
    "    sumsq_ecg += (x_ecg * x_ecg).sum().item()\n",
    "    count_ecg += x_ecg.numel()\n",
    "\n",
    "mean_low = sum_low / count_low\n",
    "std_low  = math.sqrt(max(sumsq_low / count_low - mean_low * mean_low, 0.0))\n",
    "mean_ecg = sum_ecg / count_ecg\n",
    "std_ecg  = math.sqrt(max(sumsq_ecg / count_ecg - mean_ecg * mean_ecg, 0.0))\n",
    "print(f\"[val GLOBAL] low mean/std ~ {mean_low:.4f} / {std_low:.4f}\")\n",
    "print(f\"[val GLOBAL] ecg mean/std ~ {mean_ecg:.4f} / {std_ecg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, hashlib, pathlib\n",
    "\n",
    "fold_dir = Path(cfg.data_root) / cfg.fold\n",
    "low_stats_path = str(fold_dir / \"norm_low.npz\")\n",
    "ecg_stats_path = str(fold_dir / \"norm_ecg.npz\")\n",
    "\n",
    "# 1) Confirm paths\n",
    "print(\"Using:\", low_stats_path)\n",
    "print(\"Using:\", ecg_stats_path)\n",
    "\n",
    "# 2) Print the actual arrays we used for the manual transform (and shapes/dtypes)\n",
    "sl = np.load(low_stats_path)\n",
    "se = np.load(ecg_stats_path)\n",
    "print(\"low mean:\", sl[\"mean\"], \" std:\", sl[\"std\"], \" dtype:\", sl[\"mean\"].dtype, sl[\"std\"].dtype)\n",
    "print(\"ecg mean:\", se[\"mean\"], \" std:\", se[\"std\"], \" dtype:\", se[\"mean\"].dtype, se[\"std\"].dtype)\n",
    "\n",
    "# 3) md5 so we can compare to the one you logged earlier\n",
    "def md5(p): \n",
    "    h=hashlib.md5(); h.update(pathlib.Path(p).read_bytes()); return h.hexdigest()\n",
    "print(\"low npz md5:\", md5(low_stats_path))\n",
    "print(\"ecg npz md5:\", md5(ecg_stats_path))\n",
    "\n",
    "# --- 1) Build two loaders over the same split:\n",
    "#     a) RAW (no normalization, no aug)\n",
    "raw_loader = make_loader(\n",
    "    root_dir=cfg.data_root, fold=cfg.fold, split=cfg.train_split,\n",
    "    window_size_low=cfg.seq_length_low, batch_size=cfg.batch_size,\n",
    "    shuffle=False, num_workers=0, condition_dim=cfg.condition_dim,\n",
    "    augment=False,\n",
    "    normalize=False, normalize_ecg=False,\n",
    "    expected_ecg_len=cfg.seq_length_ecg,\n",
    ")\n",
    "\n",
    "#     b) NORMED by dataset (use same .npz files, no aug)\n",
    "norm_loader = make_loader(\n",
    "    root_dir=cfg.data_root, fold=cfg.fold, split=cfg.train_split,\n",
    "    window_size_low=cfg.seq_length_low, batch_size=cfg.batch_size,\n",
    "    shuffle=False, num_workers=0, condition_dim=cfg.condition_dim,\n",
    "    augment=False,\n",
    "    normalize=True, normalize_ecg=True,\n",
    "    stats_low_path=low_stats_path,\n",
    "    stats_ecg_path=ecg_stats_path,\n",
    "    expected_ecg_len=cfg.seq_length_ecg,\n",
    "    force_use_stats=True,\n",
    "    use_split_stats_if_needed=False,\n",
    ")\n",
    "\n",
    "# --- 2) Pull one batch from each\n",
    "b_raw  = next(iter(raw_loader))\n",
    "b_norm = next(iter(norm_loader))\n",
    "\n",
    "x_raw_low = b_raw[\"signal_low\"].to(device).float()     # (B, T_low, 2)\n",
    "x_raw_ecg = b_raw[\"signal_ecg\"].to(device).float()     # (B, T_ecg, 1)\n",
    "\n",
    "x_norm_low = b_norm[\"signal_low\"].to(device).float()\n",
    "x_norm_ecg = b_norm[\"signal_ecg\"].to(device).float()\n",
    "\n",
    "# --- 3) Manual normalization using the same .npz stats\n",
    "stats_low = np.load(low_stats_path)\n",
    "mu_low = torch.as_tensor(stats_low[\"mean\"], device=device, dtype=x_raw_low.dtype).view(1,1,-1)\n",
    "sd_low = torch.as_tensor(stats_low[\"std\"],  device=device, dtype=x_raw_low.dtype).view(1,1,-1)\n",
    "\n",
    "stats_ecg = np.load(ecg_stats_path)\n",
    "mu_ecg = torch.as_tensor(stats_ecg[\"mean\"], device=device, dtype=x_raw_ecg.dtype).view(1,1,1)\n",
    "sd_ecg = torch.as_tensor(stats_ecg[\"std\"],  device=device, dtype=x_raw_ecg.dtype).view(1,1,1)\n",
    "\n",
    "x_manual_low = (x_raw_low - mu_low) / sd_low\n",
    "x_manual_ecg = (x_raw_ecg - mu_ecg) / sd_ecg\n",
    "\n",
    "# --- 4) Report means/stds and diffs\n",
    "def ms(x):\n",
    "    m = x.mean(dim=(0,1))\n",
    "    s = x.std(dim=(0,1), unbiased=False)\n",
    "    return m.tolist(), s.tolist()\n",
    "\n",
    "m_raw_low,    s_raw_low    = ms(x_raw_low)\n",
    "m_norm_low,   s_norm_low   = ms(x_norm_low)\n",
    "m_manual_low, s_manual_low = ms(x_manual_low)\n",
    "\n",
    "m_raw_ecg,    s_raw_ecg    = x_raw_ecg.mean().item(),  x_raw_ecg.std(unbiased=False).item()\n",
    "m_norm_ecg,   s_norm_ecg   = x_norm_ecg.mean().item(), x_norm_ecg.std(unbiased=False).item()\n",
    "m_manual_ecg, s_manual_ecg = x_manual_ecg.mean().item(), x_manual_ecg.std(unbiased=False).item()\n",
    "\n",
    "diff_low   = (x_manual_low - x_norm_low).abs()\n",
    "diff_ecg   = (x_manual_ecg - x_norm_ecg).abs()\n",
    "\n",
    "print(\"--- LOW (EDA/RESP) ---\")\n",
    "print(\"raw     mean≈\", m_raw_low,    \" std≈\", s_raw_low)\n",
    "print(\"manual  mean≈\", m_manual_low, \" std≈\", s_manual_low)\n",
    "print(\"loader  mean≈\", m_norm_low,   \" std≈\", s_norm_low)\n",
    "print(\"Δ(low)  max_abs≈\", diff_low.max().item(),\n",
    "      \" per-chan max_abs≈\", diff_low.flatten(0,1).max(dim=0).values.tolist())\n",
    "\n",
    "print(\"\\n--- ECG ---\")\n",
    "print(f\"raw     mean≈ {m_raw_ecg:.4f}  std≈ {s_raw_ecg:.4f}\")\n",
    "print(f\"manual  mean≈ {m_manual_ecg:.4f}  std≈ {s_manual_ecg:.4f}\")\n",
    "print(f\"loader  mean≈ {m_norm_ecg:.4f}  std≈ {s_norm_ecg:.4f}\")\n",
    "print(\"Δ(ecg)  max_abs≈\", diff_ecg.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714dd70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_channel_low_stats(loader):\n",
    "    sum_c = torch.zeros(2, dtype=torch.float64)\n",
    "    sumsq_c = torch.zeros(2, dtype=torch.float64)\n",
    "    cnt = 0\n",
    "    for b in loader:\n",
    "        x = b[\"signal_low\"].double()  # (B,T,2)\n",
    "        sum_c   += x.sum(dim=(0,1))\n",
    "        sumsq_c += (x*x).sum(dim=(0,1))\n",
    "        cnt     += x.shape[0]*x.shape[1]\n",
    "    mu = (sum_c / cnt).float()\n",
    "    var = (sumsq_c / cnt).float() - mu*mu\n",
    "    std = var.clamp_min(0).sqrt()\n",
    "    return mu.tolist(), std.tolist()\n",
    "\n",
    "mu_c, sd_c = per_channel_low_stats(val_loader)\n",
    "print(\"Per‑channel low mean:\", mu_c, \"std:\", sd_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f9f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(val_loader))\n",
    "print(\"Shapes:\",\n",
    "      \"low\", tuple(b[\"signal_low\"].shape),\n",
    "      \"ecg\", tuple(b[\"signal_ecg\"].shape),\n",
    "      \"cond\", tuple(b[\"condition\"].shape))\n",
    "\n",
    "print(\"[sanity] batch low mean/std:\",\n",
    "      b[\"signal_low\"].mean().item(), b[\"signal_low\"].std().item())\n",
    "print(\"[sanity] batch ecg mean/std:\",\n",
    "      b[\"signal_ecg\"].mean().item(), b[\"signal_ecg\"].std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a467353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def global_mean_std(loader):\n",
    "    sum_low = sumsq_low = 0.0\n",
    "    sum_ecg = sumsq_ecg = 0.0\n",
    "    n_low = n_ecg = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        x_low = batch[\"signal_low\"].double()\n",
    "        x_ecg = batch[\"signal_ecg\"].double()\n",
    "        sum_low   += x_low.sum().item()\n",
    "        sumsq_low += (x_low * x_low).sum().item()\n",
    "        n_low     += x_low.numel()\n",
    "        sum_ecg   += x_ecg.sum().item()\n",
    "        sumsq_ecg += (x_ecg * x_ecg).sum().item()\n",
    "        n_ecg     += x_ecg.numel()\n",
    "\n",
    "    mu_low = sum_low / n_low\n",
    "    sd_low = math.sqrt(max(sumsq_low / n_low - mu_low * mu_low, 0.0))\n",
    "    mu_ecg = sum_ecg / n_ecg\n",
    "    sd_ecg = math.sqrt(max(sumsq_ecg / n_ecg - mu_ecg * mu_ecg, 0.0))\n",
    "    return mu_low, sd_low, mu_ecg, sd_ecg\n",
    "\n",
    "mu_l, sd_l, mu_e, sd_e = global_mean_std(val_loader)\n",
    "print(f\"[val GLOBAL] low mean/std ~ {mu_l:.4f} / {sd_l:.4f}\")\n",
    "print(f\"[val GLOBAL] ecg mean/std ~ {mu_e:.4f} / {sd_e:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e043e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_low = float(getattr(cfg, \"boundary_margin_low\", 0.92))\n",
    "ecg_margin = float(getattr(cfg, \"ecg_margin\", 0.98))\n",
    "\n",
    "def clamp_fraction(loader, margin_low, ecg_margin, n_batches=5):\n",
    "    frac_low = []; frac_ecg = []\n",
    "    k = 0\n",
    "    for b in loader:\n",
    "        x_low = b[\"signal_low\"]    # (B,T,2)\n",
    "        x_ecg = b[\"signal_ecg\"]    # (B,T,1)\n",
    "\n",
    "        f_low = (x_low.abs() >= margin_low).float().mean().item()\n",
    "        f_ecg = (x_ecg.abs() >= ecg_margin).float().mean().item()\n",
    "\n",
    "        frac_low.append(f_low); frac_ecg.append(f_ecg)\n",
    "        k += 1\n",
    "        if k >= n_batches:\n",
    "            break\n",
    "    return np.mean(frac_low), np.mean(frac_ecg)\n",
    "\n",
    "fr_low, fr_ecg = clamp_fraction(val_loader, margin_low, ecg_margin)\n",
    "print(f\"[clamp fraction] low≈{fr_low:.3f}, ecg≈{fr_ecg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5117c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = np.load(fold_dir / \"norm_low.npz\")\n",
    "mu, sd = st[\"mean\"].copy(), st[\"std\"].copy()\n",
    "mu_sw, sd_sw = mu[[1,0]], sd[[1,0]]\n",
    "np.savez(fold_dir / \"norm_low.npz\", mean=mu_sw, std=sd_sw, channels=np.array([\"EDA\",\"RESP\"]))\n",
    "print(\"Swapped channels in norm_low.npz; rebuild loaders and re-check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622e3914",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model init ---\n",
    "G, D = create_tc_multigan(cfg)\n",
    "G.to(device); D.to(device)\n",
    "\n",
    "# Safety freeze for fixed smoothing convs (even if model file wasn't edited)\n",
    "def _freeze_if_exists(module, attr):\n",
    "    if hasattr(module, attr):\n",
    "        m = getattr(module, attr)\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "for m in getattr(G, \"ecg_up\", []):\n",
    "    if hasattr(m, \"smooth\"):\n",
    "        for p in m.smooth.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "for name in (\"low_up1\", \"low_up2\"):\n",
    "    if hasattr(G, name):\n",
    "        _freeze_if_exists(getattr(G, name), \"smooth\")\n",
    "\n",
    "_freeze_if_exists(G, \"final_smooth\")\n",
    "print(\"[init] Fixed smoothing kernels set to requires_grad=False (safety freeze).\")\n",
    "\n",
    "# --- EMA (optional) ---\n",
    "use_ema = bool(getattr(cfg, \"use_ema\", False))\n",
    "ema = EMA(G, decay=float(getattr(cfg, \"ema_decay\", 0.999))) if use_ema else None\n",
    "\n",
    "# --- Losses / optimizers / schedulers ---\n",
    "criterion_aux = nn.CrossEntropyLoss()\n",
    "\n",
    "opt_g = optim.Adam(G.parameters(), lr=cfg.lr_g, betas=(0.5, 0.999))\n",
    "opt_d = optim.Adam(D.parameters(), lr=cfg.lr_d, betas=(0.5, 0.999))\n",
    "\n",
    "for opt in (opt_g, opt_d):\n",
    "    for pg in opt.param_groups:\n",
    "        if 'initial_lr' not in pg:\n",
    "            pg['initial_lr'] = pg['lr']\n",
    "\n",
    "sched_g = CosineAnnealingLR(opt_g, T_max=cfg.epochs_gan, last_epoch=-1)\n",
    "sched_d = CosineAnnealingLR(opt_d, T_max=cfg.epochs_gan, last_epoch=-1)\n",
    "\n",
    "# --- Dirs ---\n",
    "os.makedirs(cfg.ckpt_dir, exist_ok=True)\n",
    "os.makedirs(cfg.sample_dir, exist_ok=True)\n",
    "os.makedirs(cfg.log_dir, exist_ok=True)\n",
    "\n",
    "# --- Fixed noise / cond for sampling ---\n",
    "n_samples = cfg.sample_n\n",
    "g = torch.Generator(device=device).manual_seed(cfg.seed)\n",
    "fixed_noise = torch.randn(n_samples, cfg.z_dim, generator=g, device=device)\n",
    "\n",
    "# Build deterministic condition sequences from present classes (or all K)\n",
    "present = None\n",
    "try:\n",
    "    ds = train_loader.dataset\n",
    "    if getattr(ds, \"cond_np\", None) is not None:\n",
    "        present = np.unique(ds.cond_np)\n",
    "        if present.min() == 1 and present.max() == cfg.condition_dim:\n",
    "            present = present - 1  # map 1..K -> 0..K-1 if needed\n",
    "        present = [int(c) for c in present if 0 <= c < cfg.condition_dim]\n",
    "except Exception:\n",
    "    present = None\n",
    "if not present:\n",
    "    present = list(range(cfg.condition_dim))\n",
    "\n",
    "fixed_cond_low = torch.zeros(n_samples, cfg.seq_length_low, cfg.condition_dim, device=device)\n",
    "for i in range(n_samples):\n",
    "    c = present[i % len(present)]\n",
    "    fixed_cond_low[i, :, c] = 1.0\n",
    "\n",
    "print(\"[init] Model/opt/sched/EMA ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bafd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "import torch\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Sensible defaults if not present in cfg\n",
    "if not hasattr(cfg, \"eval_interval\"):    cfg.eval_interval    = 5\n",
    "if not hasattr(cfg, \"eval_n_batches\"):   cfg.eval_n_batches   = 8\n",
    "if not hasattr(cfg, \"eval_bootstrap\"):   cfg.eval_bootstrap   = 0      # set 200 for CI95\n",
    "if not hasattr(cfg, \"eval_channels\"):    cfg.eval_channels    = (\"eda\",\"resp\",\"ecg\")\n",
    "if not hasattr(cfg, \"eval_viz_n\"):       cfg.eval_viz_n       = 4\n",
    "if not hasattr(cfg, \"eval_use_clamp\"):   cfg.eval_use_clamp   = True   # match D's observed view\n",
    "if not hasattr(cfg, \"eval_standardize\"): cfg.eval_standardize = False  # compare in native units\n",
    "if not hasattr(cfg, \"probe_n_train\"):    cfg.probe_n_train    = 2000\n",
    "if not hasattr(cfg, \"probe_n_val\"):      cfg.probe_n_val      = 1000\n",
    "\n",
    "epoch_times = []\n",
    "\n",
    "for epoch in range(1, cfg.epochs_gan + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ---- Train ----\n",
    "    loss_g, loss_d = train_one_epoch(\n",
    "        G, D, train_loader, opt_g, opt_d,\n",
    "        device, cfg, criterion_aux, epoch,\n",
    "        ema=ema, use_ema=bool(cfg.use_ema)\n",
    "    )\n",
    "\n",
    "    # ---- Validate (with EMA symmetry if enabled) ----\n",
    "    if bool(cfg.use_ema) and (ema is not None):\n",
    "        ema.apply_to(G)\n",
    "    # NOTE: validate signature: (..., _criterion_adv_unused, criterion_aux)\n",
    "    val_loss_g, val_loss_d = validate(G, D, val_loader, device, cfg, criterion_aux)\n",
    "    if bool(cfg.use_ema) and (ema is not None):\n",
    "        ema.restore(G)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | Train G: {loss_g:.4f}, D: {loss_d:.4f} | \"\n",
    "          f\"Val G: {val_loss_g:.4f}, D: {val_loss_d:.4f}\")\n",
    "\n",
    "    # ---- Distribution + Label-Structure Evaluation (on schedule) ----\n",
    "    if (epoch % int(cfg.eval_interval) == 0) or (epoch == cfg.epochs_gan):\n",
    "        try:\n",
    "            summary = run_epoch_evaluations(\n",
    "                G=G, D=D,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                cfg=cfg, device=device,\n",
    "                epoch=epoch, ema=ema\n",
    "            )\n",
    "\n",
    "            # compact headline for logs\n",
    "            parts = []\n",
    "            for k in (\"ks_eda\", \"w1_ecg\", \"js_resp\",\n",
    "                    \"probe_acc_real\", \"probe_acc_fake\",\n",
    "                    \"probe_bacc_real\", \"probe_bacc_fake\",\n",
    "                    \"probe_f1_real\", \"probe_f1_fake\"):\n",
    "                v = summary.get(k)\n",
    "                if v is not None:\n",
    "                    parts.append(f\"{k}={v:.3f}\")\n",
    "            if parts:\n",
    "                print(\"[eval] \" + \" | \".join(parts))\n",
    "        except Exception as e:\n",
    "            print(f\"[eval] Skipped this epoch due to error: {e}\")\n",
    "\n",
    "    # ---- Sampling (fixed grid) ----\n",
    "    if epoch % int(cfg.sample_interval) == 0:\n",
    "        if bool(cfg.use_ema) and (ema is not None):\n",
    "            ema.apply_to(G)\n",
    "        generate_and_save_samples(G, fixed_noise, fixed_cond_low, cfg, epoch)\n",
    "        if bool(cfg.use_ema) and (ema is not None):\n",
    "            ema.restore(G)\n",
    "\n",
    "    # ---- Schedulers ----\n",
    "    sched_g.step()\n",
    "    sched_d.step()\n",
    "\n",
    "    # ---- Checkpointing ----\n",
    "    if (epoch % int(cfg.ckpt_interval) == 0) or (val_loss_d < best_val_loss):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'G_state_dict': G.state_dict(),\n",
    "            'D_state_dict': D.state_dict(),\n",
    "            'opt_g_state_dict': opt_g.state_dict(),\n",
    "            'opt_d_state_dict': opt_d.state_dict(),\n",
    "            'ema_state_dict': (ema.shadow if (ema is not None) else None),\n",
    "        }\n",
    "        ckpt_path = os.path.join(cfg.ckpt_dir, f\"ckpt_epoch_{epoch:03d}.pt\")\n",
    "        torch.save(checkpoint, ckpt_path)\n",
    "        best_val_loss = min(best_val_loss, val_loss_d)\n",
    "\n",
    "    # ---- Epoch timing (optional) ----\n",
    "    epoch_times.append(time.time() - t0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
