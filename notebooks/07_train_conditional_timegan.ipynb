{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03b25959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Dynamically add root directory (project base)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a149648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import mixed_precision\n",
    "from models.timegan.timegan_model import TimeGAN\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c8b4ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… X shape: (55917, 256, 12)\n"
     ]
    }
   ],
   "source": [
    "# Load your processed data\n",
    "X = np.load(\"../data/processed/chest_X_conditional.npy\")\n",
    "print(f\"âœ… X shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "30f1d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 256\n",
    "feature_dim = 12  # 8 signals + 4 class indicators\n",
    "hidden_dim = 16\n",
    "num_layers = 2\n",
    "iterations = 10000\n",
    "batch_size = 64\n",
    "gamma = 1  # weight on supervised loss\n",
    "\n",
    "\n",
    "timegan = TimeGAN(seq_len, feature_dim, hidden_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2b892496",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "generator_optimizer = mixed_precision.LossScaleOptimizer(\n",
    "    tf.keras.optimizers.Adam(learning_rate=1e-4), dynamic=True)\n",
    "discriminator_optimizer = mixed_precision.LossScaleOptimizer(\n",
    "    tf.keras.optimizers.Adam(learning_rate=1e-4), dynamic=True)\n",
    "embedder_optimizer = mixed_precision.LossScaleOptimizer(\n",
    "    tf.keras.optimizers.Adam(learning_rate=1e-4), dynamic=True)\n",
    "supervisor_optimizer = mixed_precision.LossScaleOptimizer(\n",
    "    tf.keras.optimizers.Adam(learning_rate=1e-4), dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd150dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_noise(batch_size, seq_len, dim):\n",
    "    return tf.random.normal(shape=(batch_size, seq_len, dim), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "95a8f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(jit_compile=True)\n",
    "def train_embedder(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = timegan.embedder(x)\n",
    "        x_tilde = timegan.recovery(h)\n",
    "        e_loss = mse(x, x_tilde)\n",
    "\n",
    "    grads = tape.gradient(e_loss, timegan.embedder.trainable_variables + timegan.recovery.trainable_variables)\n",
    "    embedder_optimizer.apply_gradients(zip(grads, timegan.embedder.trainable_variables + timegan.recovery.trainable_variables))\n",
    "    return e_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "36263c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(jit_compile=True)\n",
    "def train_supervisor(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = timegan.embedder(x)\n",
    "        h_supervised = timegan.supervisor(h)\n",
    "        s_loss = mse(h[:, 1:, :], h_supervised[:, :-1, :])  # next-step prediction\n",
    "\n",
    "    grads = tape.gradient(s_loss, timegan.supervisor.trainable_variables)\n",
    "    supervisor_optimizer.apply_gradients(zip(grads, timegan.supervisor.trainable_variables))\n",
    "\n",
    "    return s_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "22d62b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(jit_compile=True)\n",
    "def train_generator(x):\n",
    "    z = sample_random_noise(tf.shape(x)[0], seq_len, hidden_dim)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        e_hat = timegan.generator(z)\n",
    "        h_hat = timegan.supervisor(e_hat)\n",
    "        y_fake = timegan.discriminator(h_hat)\n",
    "\n",
    "        g_loss_u = bce(tf.ones_like(y_fake), y_fake)  # adversarial loss\n",
    "        g_loss_s = mse(h_hat[:, 1:, :], timegan.supervisor(h_hat)[:, :-1, :])  # supervised loss\n",
    "        g_loss = g_loss_u + gamma * g_loss_s\n",
    "\n",
    "    grads = tape.gradient(g_loss, timegan.generator.trainable_variables + timegan.supervisor.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(grads, timegan.generator.trainable_variables + timegan.supervisor.trainable_variables))\n",
    "\n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4671ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(jit_compile=True)\n",
    "def train_discriminator(x):\n",
    "    h = timegan.embedder(x)\n",
    "    z = sample_random_noise(tf.shape(x)[0], seq_len, hidden_dim)\n",
    "    h_hat = timegan.generate_latent(z)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_real = timegan.discriminator(h)\n",
    "        y_fake = timegan.discriminator(tf.stop_gradient(h_hat))\n",
    "\n",
    "        d_loss_real = bce(tf.ones_like(y_real), y_real)\n",
    "        d_loss_fake = bce(tf.zeros_like(y_fake), y_fake)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "    grads = tape.gradient(d_loss, timegan.discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(grads, timegan.discriminator.trainable_variables))\n",
    "\n",
    "    return d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "54eea340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have a model like timegan\n",
    "# You create fake (zero) data to initialize it:\n",
    "\n",
    "# 1. This calls the embedder + recovery to get x_tilde\n",
    "_ = timegan(tf.zeros((1, seq_len, feature_dim)))\n",
    "\n",
    "# 2. This initializes the generator + supervisor (latent space modules)\n",
    "_ = timegan.generate_latent(tf.zeros((1, seq_len, hidden_dim)))\n",
    "\n",
    "# 3. This initializes the discriminator\n",
    "_ = timegan.discriminator(tf.zeros((1, seq_len, hidden_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "989c9192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ†• Initializing from scratch.\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint setup for saving and restoring\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    generator=timegan.generator,\n",
    "    supervisor=timegan.supervisor,\n",
    "    discriminator=timegan.discriminator,\n",
    "    embedder=timegan.embedder,\n",
    "    generator_optimizer=generator_optimizer,\n",
    "    discriminator_optimizer=discriminator_optimizer,\n",
    "    embedder_optimizer=embedder_optimizer,\n",
    "    supervisor_optimizer=supervisor_optimizer\n",
    ")\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, './checkpoints/timegan', max_to_keep=3)\n",
    "\n",
    "# Optionally restore the latest checkpoint if it exists\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(f\"âœ… Restored from checkpoint: {ckpt_manager.latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"ðŸ†• Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "62f1b9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting TimeGAN training...\n",
      "[Step 0] Embedder loss: 0.5026\n",
      "[Step 100] Embedder loss: 0.5029\n",
      "[Step 200] Embedder loss: 0.4757\n",
      "[Step 300] Embedder loss: 0.5031\n",
      "[Step 400] Embedder loss: 0.5427\n",
      "[Step 500] Embedder loss: 0.4926\n",
      "[Step 600] Embedder loss: 0.5059\n",
      "[Step 700] Embedder loss: 0.4908\n",
      "[Step 800] Embedder loss: 0.4353\n",
      "[Step 900] Embedder loss: 0.5116\n",
      "[Step 1000] Supervisor loss: 0.0258\n",
      "[Step 1100] Supervisor loss: 0.0246\n",
      "[Step 1200] Supervisor loss: 0.0229\n",
      "[Step 1300] Supervisor loss: 0.0198\n",
      "[Step 1400] Supervisor loss: 0.0195\n",
      "[Step 1500] Supervisor loss: 0.0186\n",
      "[Step 1600] Supervisor loss: 0.0173\n",
      "[Step 1700] Supervisor loss: 0.0172\n",
      "[Step 1800] Supervisor loss: 0.0180\n",
      "[Step 1900] Supervisor loss: 0.0164\n",
      "[Step 2000] Generator loss: 0.7103, Discriminator loss: 1.4533\n",
      "âœ… Saved checkpoint at step 2000\n",
      "[Step 2100] Generator loss: 0.6922, Discriminator loss: 1.3625\n",
      "[Step 2200] Generator loss: 0.6984, Discriminator loss: 1.2727\n",
      "[Step 2300] Generator loss: 0.7010, Discriminator loss: 1.2019\n",
      "[Step 2400] Generator loss: 0.6759, Discriminator loss: 1.1907\n",
      "[Step 2500] Generator loss: 0.6229, Discriminator loss: 1.2685\n",
      "[Step 2600] Generator loss: 0.6088, Discriminator loss: 1.3547\n",
      "[Step 2700] Generator loss: 0.6908, Discriminator loss: 1.3217\n",
      "[Step 2800] Generator loss: 0.8013, Discriminator loss: 1.2185\n",
      "[Step 2900] Generator loss: 0.9142, Discriminator loss: 1.0942\n",
      "[Step 3000] Generator loss: 0.9117, Discriminator loss: 1.1550\n",
      "âœ… Saved checkpoint at step 3000\n",
      "[Step 3100] Generator loss: 0.6132, Discriminator loss: 1.6689\n",
      "[Step 3200] Generator loss: 0.6065, Discriminator loss: 1.6196\n",
      "[Step 3300] Generator loss: 0.6736, Discriminator loss: 1.5533\n",
      "[Step 3400] Generator loss: 0.7065, Discriminator loss: 1.5263\n",
      "[Step 3500] Generator loss: 0.7475, Discriminator loss: 1.4694\n",
      "[Step 3600] Generator loss: 0.7727, Discriminator loss: 1.4270\n",
      "[Step 3700] Generator loss: 0.7796, Discriminator loss: 1.3956\n",
      "[Step 3800] Generator loss: 0.7764, Discriminator loss: 1.3738\n",
      "[Step 3900] Generator loss: 0.7704, Discriminator loss: 1.3585\n",
      "[Step 4000] Generator loss: 0.7609, Discriminator loss: 1.3495\n",
      "âœ… Saved checkpoint at step 4000\n",
      "[Step 4100] Generator loss: 0.7486, Discriminator loss: 1.3563\n",
      "[Step 4200] Generator loss: 0.7292, Discriminator loss: 1.3811\n",
      "[Step 4300] Generator loss: 0.7105, Discriminator loss: 1.4063\n",
      "[Step 4400] Generator loss: 0.7057, Discriminator loss: 1.4023\n",
      "[Step 4500] Generator loss: 0.7090, Discriminator loss: 1.3730\n",
      "[Step 4600] Generator loss: 0.7020, Discriminator loss: 1.3555\n",
      "[Step 4700] Generator loss: 0.6756, Discriminator loss: 1.3891\n",
      "[Step 4800] Generator loss: 0.6587, Discriminator loss: 1.4218\n",
      "[Step 4900] Generator loss: 0.6689, Discriminator loss: 1.4029\n",
      "[Step 5000] Generator loss: 0.6770, Discriminator loss: 1.3808\n",
      "âœ… Saved checkpoint at step 5000\n",
      "[Step 5100] Generator loss: 0.6666, Discriminator loss: 1.4042\n",
      "[Step 5200] Generator loss: 0.6680, Discriminator loss: 1.4245\n",
      "[Step 5300] Generator loss: 0.6930, Discriminator loss: 1.4017\n",
      "[Step 5400] Generator loss: 0.7146, Discriminator loss: 1.3680\n",
      "[Step 5500] Generator loss: 0.7146, Discriminator loss: 1.3658\n",
      "[Step 5600] Generator loss: 0.7077, Discriminator loss: 1.3884\n",
      "[Step 5700] Generator loss: 0.7102, Discriminator loss: 1.3896\n",
      "[Step 5800] Generator loss: 0.6855, Discriminator loss: 1.3992\n",
      "[Step 5900] Generator loss: 0.6682, Discriminator loss: 1.4055\n",
      "[Step 6000] Generator loss: 0.6798, Discriminator loss: 1.3931\n",
      "âœ… Saved checkpoint at step 6000\n",
      "[Step 6100] Generator loss: 0.6866, Discriminator loss: 1.3887\n",
      "[Step 6200] Generator loss: 0.6910, Discriminator loss: 1.3868\n",
      "[Step 6300] Generator loss: 0.6942, Discriminator loss: 1.3874\n",
      "[Step 6400] Generator loss: 0.6969, Discriminator loss: 1.3866\n",
      "[Step 6500] Generator loss: 0.6986, Discriminator loss: 1.3874\n",
      "[Step 6600] Generator loss: 0.7005, Discriminator loss: 1.3880\n",
      "[Step 6700] Generator loss: 0.7019, Discriminator loss: 1.3890\n",
      "[Step 6800] Generator loss: 0.7037, Discriminator loss: 1.3896\n",
      "[Step 6900] Generator loss: 0.7053, Discriminator loss: 1.3894\n",
      "[Step 7000] Generator loss: 0.7066, Discriminator loss: 1.3888\n",
      "âœ… Saved checkpoint at step 7000\n",
      "[Step 7100] Generator loss: 0.7072, Discriminator loss: 1.3869\n",
      "[Step 7200] Generator loss: 0.7060, Discriminator loss: 1.3861\n",
      "[Step 7300] Generator loss: 0.7034, Discriminator loss: 1.3861\n",
      "[Step 7400] Generator loss: 0.6992, Discriminator loss: 1.3866\n",
      "[Step 7500] Generator loss: 0.6954, Discriminator loss: 1.3856\n",
      "[Step 7600] Generator loss: 0.6917, Discriminator loss: 1.3841\n",
      "[Step 7700] Generator loss: 0.6881, Discriminator loss: 1.3829\n",
      "[Step 7800] Generator loss: 0.6849, Discriminator loss: 1.3828\n",
      "[Step 7900] Generator loss: 0.6830, Discriminator loss: 1.3826\n",
      "[Step 8000] Generator loss: 0.6825, Discriminator loss: 1.3828\n",
      "âœ… Saved checkpoint at step 8000\n",
      "[Step 8100] Generator loss: 0.6827, Discriminator loss: 1.3863\n",
      "[Step 8200] Generator loss: 0.6834, Discriminator loss: 1.3891\n",
      "[Step 8300] Generator loss: 0.6845, Discriminator loss: 1.3897\n",
      "[Step 8400] Generator loss: 0.6862, Discriminator loss: 1.3903\n",
      "[Step 8500] Generator loss: 0.6880, Discriminator loss: 1.3925\n",
      "[Step 8600] Generator loss: 0.6903, Discriminator loss: 1.3926\n",
      "[Step 8700] Generator loss: 0.6923, Discriminator loss: 1.3924\n",
      "[Step 8800] Generator loss: 0.6946, Discriminator loss: 1.3923\n",
      "[Step 8900] Generator loss: 0.6965, Discriminator loss: 1.3921\n",
      "[Step 9000] Generator loss: 0.6985, Discriminator loss: 1.3909\n",
      "âœ… Saved checkpoint at step 9000\n",
      "[Step 9100] Generator loss: 0.7001, Discriminator loss: 1.3900\n",
      "[Step 9200] Generator loss: 0.7014, Discriminator loss: 1.3890\n",
      "[Step 9300] Generator loss: 0.7023, Discriminator loss: 1.3878\n",
      "[Step 9400] Generator loss: 0.7027, Discriminator loss: 1.3867\n",
      "[Step 9500] Generator loss: 0.7029, Discriminator loss: 1.3865\n",
      "[Step 9600] Generator loss: 0.7029, Discriminator loss: 1.3870\n",
      "[Step 9700] Generator loss: 0.7026, Discriminator loss: 1.3864\n",
      "[Step 9800] Generator loss: 0.7020, Discriminator loss: 1.3858\n",
      "[Step 9900] Generator loss: 0.7014, Discriminator loss: 1.3868\n",
      "ðŸš€ TimeGAN training End\n"
     ]
    }
   ],
   "source": [
    "X_train = tf.convert_to_tensor(X.astype(np.float32))\n",
    "\n",
    "log_dir = \"./logs/timegan\"\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# Logging and checkpoints\n",
    "log_every = 100\n",
    "save_every = 1000\n",
    "g_losses, d_losses, e_losses, s_losses = [], [], [], []\n",
    "\n",
    "print(\"ðŸš€ Starting TimeGAN training...\")\n",
    "\n",
    "for step in range(iterations):\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    x_batch = tf.gather(X_train, idx)\n",
    "    x_batch.set_shape([batch_size, seq_len, feature_dim])\n",
    "\n",
    "    # Phase 1: Embedder training\n",
    "    if step < 1000:\n",
    "        e_loss = train_embedder(x_batch)\n",
    "        if step % log_every == 0:\n",
    "            print(f\"[Step {step}] Embedder loss: {e_loss.numpy():.4f}\")\n",
    "            e_losses.append((step, e_loss.numpy()))\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar(\"Embedder Loss\", e_loss, step=step)\n",
    "        continue\n",
    "\n",
    "    # Phase 2: Supervisor training\n",
    "    if step < 2000:\n",
    "        s_loss = train_supervisor(x_batch)\n",
    "        if step % log_every == 0:\n",
    "            print(f\"[Step {step}] Supervisor loss: {s_loss.numpy():.4f}\")\n",
    "            s_losses.append((step, s_loss.numpy()))\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar(\"Supervisor Loss\", s_loss, step=step)\n",
    "        continue\n",
    "\n",
    "    # Phase 3: Joint training\n",
    "    g_loss = train_generator(x_batch)\n",
    "    d_loss = train_discriminator(x_batch)\n",
    "\n",
    "    if step % log_every == 0:\n",
    "        print(f\"[Step {step}] Generator loss: {g_loss:.4f}, Discriminator loss: {d_loss:.4f}\")\n",
    "        g_losses.append((step, float(g_loss)))\n",
    "        d_losses.append((step, float(d_loss)))\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"Generator Loss\", g_loss, step=step)\n",
    "            tf.summary.scalar(\"Discriminator Loss\", d_loss, step=step)\n",
    "\n",
    "    # Optional model saving\n",
    "    if step % save_every == 0 and step != 0:\n",
    "        ckpt_manager.save()\n",
    "        print(f\"âœ… Saved checkpoint at step {step}\")\n",
    "\n",
    "print(\"ðŸš€ TimeGAN training End\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
